# Configuration for the admins service
admins:
  # name specifies the name identifier for the admins service components.
  name: admins
  # team specifies the team associated with this service.
  team: frontegg
  # envID specifies the environment identifier (e.g., local, dev, prod).
  envID: prod

  image:
    # repository specifies the Docker image repository for the admins service.
    repository: frontegg/admins-service
  # web contains configurations specific to the web deployment of the admins service.
  web:
    # enabled specifies whether the web component of the admins service is enabled.
    enabled: true
    # command overrides the default entrypoint command for the container.
    command: [ "/bin/bash" ]
    # args specifies the arguments passed to the command.
    args: [ "entrypoint.sh" ]
    # resources specifies the CPU and memory resource requests and limits for the container.
    resources:
      # requests specifies the minimum resources required.
      requests:
        # cpu specifies the requested CPU amount.
        cpu: 500m
        # memory specifies the requested memory amount.
        memory: 1Gi
      # limits specifies the maximum resources allowed.
      limits:
        # memory specifies the memory limit.
        memory: 1Gi
    # service defines the Kubernetes service configuration for the web component.
    service:
      # ports defines the ports exposed by the service.
      ports:
        # port is the port number the service listens on.
        - port: 80
          # targetPort is the port on the pod that the service forwards traffic to.
          targetPort: 3004
          # protocol specifies the network protocol (TCP or UDP).
          protocol: TCP
          # name is the name of the port.
          name: http
    # ports defines the container ports to open.
    ports:
      # name is the name of the container port.
      - name: http
        # containerPort is the port number inside the container.
        containerPort: 3004
    # autoscaling configures Horizontal Pod Autoscaler (HPA) for the web deployment.
    autoscaling:
      # enabled specifies whether HPA is enabled.
      enabled: true
      # minReplicas specifies the minimum number of replicas.
      minReplicas: 2
      # maxReplicas specifies the maximum number of replicas.
      maxReplicas: 10
      # targetCPUUtilizationPercentage specifies the target average CPU utilization across all pods.
      targetCPUUtilizationPercentage: 50
      # targetMemoryUtilizationPercentage specifies the target average memory utilization across all pods.
      targetMemoryUtilizationPercentage: 50
    # livenessProbe configures the liveness probe to check if the container is running.
    livenessProbe:
      # failureThreshold specifies how many times the probe can fail before the container is restarted.
      failureThreshold: 3
      # httpGet specifies an HTTP GET request to perform for the probe.
      httpGet:
        # path specifies the URL path to access on the container.
        path: /health
        # port specifies the name or number of the port to access on the container.
        port: http
      # initialDelaySeconds specifies the number of seconds after the container has started before the probe is initiated.
      initialDelaySeconds: 20
      # periodSeconds specifies how often (in seconds) to perform the probe.
      periodSeconds: 10
    # readinessProbe configures the readiness probe to check if the container is ready to serve traffic.
    readinessProbe:
      # httpGet specifies an HTTP GET request to perform for the probe.
      httpGet:
        # path specifies the URL path to access on the container.
        path: /health
        # port specifies the name or number of the port to access on the container.
        port: http
      # initialDelaySeconds specifies the number of seconds after the container has started before the probe is initiated.
      initialDelaySeconds: 5
      # periodSeconds specifies how often (in seconds) to perform the probe.
      periodSeconds: 5
  # configmap defines the configuration map for the admins service.
  configmap:
    # data contains the key-value pairs for the configmap.
    data:
      # CLOUD_ENVIRONMENT specifies the cloud environment type.
      CLOUD_ENVIRONMENT: production
      # NODE_ENV specifies the Node.js environment.
      NODE_ENV: production
      # FRONTEGG_CONFIG_FILE_PATH specifies the path to the Frontegg configuration file.
      FRONTEGG_CONFIG_FILE_PATH: "/etc/config/config.env"

  configuration:
    map:
      config-center:
        FRONTEGG_VENDORS_SERVICE_URL: vendors-service-url
        FRONTEGG_IDENTITY_SERVICE_URL: identity-service-url
        FRONTEGG_TENANTS_SERVICE_URL: tenants-service-url
        PORTAL_URL: portal-url
        FRONTEGG_KAFKA_VENDORS_TOPIC_NAME: data-ingest-kafka-vendors-topic

  externalSecret:
    enabled: true
    mountPath: /etc/config/config.env
    text: |
      {{- $secret := .contents | fromYaml }}
      FRONTEGG_EVENT_SERVICE_API_KEY={{ $secret.frontegg.apiKeys.eventsServiceApiKey | toYaml }}
      FRONTEGG_ADMINS_SERVICE_API_KEY={{ $secret.frontegg.apiKeys.adminsServiceApiKey | toYaml }}
      FRONTEGG_VENDORS_SERVICE_API_KEY={{ $secret.frontegg.apiKeys.vendorsServiceApiKey | toYaml }}
      FRONTEGG_IDENTITY_SERVICE_API_KEY={{ $secret.frontegg.apiKeys.identityServiceApiKey | toYaml }}
      FRONTEGG_CLIENT_ID={{ $secret.frontegg.xxx.fronteggClientId | toYaml }}
      FRONTEGG_KAFKA_BROKER_LIST={{ $secret.databases.kafka.brokerList | toYaml }}
      FRONTEGG_KAFKA_SASL_PASSWORD={{ $secret.databases.kafka.saslPassword | toYaml }}
      FRONTEGG_KAFKA_SASL_USERNAME={{ $secret.databases.kafka.saslUserName | toYaml }}
      FRONTEGG_SPLIT_IO_KEY={{ $secret.externalServices.split.sdkKey | toYaml }}
      FRONTEGG_TENANTS_SERVICE_API_KEY={{ $secret.frontegg.apiKeys.tenantsServiceApiKey | toYaml }}
    additionalSecrets: ""

############## SPLIT #############

webhook-service:
  name: webhooks
  team: adoption

  envID: local

  image:
    repository: frontegg/webhook-service

  imagePullSecrets:
    - name: regcred

  web:
    enabled: true
    command: ["/bin/bash"]
    args: ["entrypoint.sh", "service"]
    service:
      ports:
        - port: 80
          targetPort: 3013
          protocol: TCP
          name: http
    ports:
      - name: http
        containerPort: 3013
    env:
      - name: FRONTEGG_IS_SERVICE_OFFLINE_PROCESS
        value: "false"
    podAnnotations:
      config.linkerd.io/skip-outbound-ports: "8080"
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        memory: 1Gi
    autoscaling:
      enabled: true
      minReplicas: 1
      maxReplicas: 2
      targetCPUUtilizationPercentage: 50
      targetMemoryUtilizationPercentage: 75
    readinessProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 20
      periodSeconds: 5
      failureThreshold: 3
    livenessProbe:
      httpGet:
        path: /health
        port: http
      failureThreshold: 30
      periodSeconds: 10
    startupProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 10
      periodSeconds: 10
      failureThreshold: 15

  worker:
    enabled: true
    command: ["/bin/bash"]
    args: ["entrypoint.sh", "worker"]
    service:
      ports:
        - port: 80
          targetPort: 3013
          protocol: TCP
          name: http
    ports:
      - name: http
        containerPort: 3013
    env:
      - name: FRONTEGG_IS_SERVICE_OFFLINE_PROCESS
        value: "true"
    podAnnotations:
      config.linkerd.io/skip-outbound-ports: "8080"
    labels:
      scrape-for-metrics: enabled
    autoscaling:
      enabled: true
      minReplicas: 1
      maxReplicas: 2
      targetCPUUtilizationPercentage: 50
      targetMemoryUtilizationPercentage: 75
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        memory: 1Gi
    readinessProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 20
      periodSeconds: 5
    startupProbe:
      httpGet:
        path: /health
        port: http
      failureThreshold: 30
      periodSeconds: 10
    livenessProbe:
      httpGet:
        path: /health
        port: http
      periodSeconds: 10
      failureThreshold: 30

  configmap:
    data:
      NODE_ENV: production
      FRONTEGG_PORT: "3013"
      FRONTEGG_SERVICE_NAME: webhook-service
      FRONTEGG_WEBHOOK_SERVICE_MONGODB_CONNECTION_RETRY_ATTEMPTS: "3"
      FRONTEGG_WEBHOOK_SERVICE_MONGODB_CONNECTION_RETRY_DELAY: "1000"
      FRONTEGG_EVENT_SERVICE_URL: http://frontegg-event-service
      FRONTEGG_EVENTS_PUBSUB_TOPIC: events
      CLOUD_ENVIRONMENT: dev
      FRONTEGG_JAEGER_ENABLED: "true"
      FRONTEGG_USE_FIREHOSE_ANALYTICS: "true"
      FRONTEGG_ANALYTICS_TYPE: firehose
      FRONTEGG_VALIDATION_WEBHOOK_URL_HOSTNAME_BLACKLIST_REGEXP: (169\.254\.\d+.\d+)|(127\.0\.0\.1)
      FRONTEGG_WEBHOOK_GUARD_FEATURE_FLAG_NAME: "webhooks_blacklist"
      FRONTEGG_HTTP_CLIENT_PROXY_AUTH_ENABLED: "false"
      FRONTEGG_CONFIG_DIRECTORY: /etc/config
      NODE_NO_WARNINGS: "1"

  jobs:
    migrate:
      enabled: true
      spec:
        command: ["/bin/bash"]
        args: ["run-migrations.sh"]
        resources:
          requests:
            cpu: 500m
            memory: 500Mi

  configuration:
    map:
      config-center:
        FRONTEGG_EVENT_SERVICE_URL: events-service-url
        FRONTEGG_JAEGER_ENDPOINT: tracing-collector-endpoint
        FRONTEGG_WEBHOOK_MYSQL_DB_NAME: webhook-mysql-db-name
        FRONTEGG_WEBHOOK_SERVICE_MONGODB_CONNECTION_RETRY_ATTEMPTS: webhook-service-mongodb-connection-retry-attempts
        FRONTEGG_WEBHOOK_SERVICE_MONGODB_CONNECTION_RETRY_DELAY: webhook-service-mongodb-connection-retry-delay
        FRONTEGG_EVENTS_PUBSUB_TOPIC: events-pubsub-topic
        FRONTEGG_HTTP_CLIENT_PROXY_ENABLED: webhook-proxy-enabled
        FRONTEGG_HTTP_CLIENT_PROXY_HOST: frontegg-proxy-host
        FRONTEGG_HTTP_CLIENT_PROXY_PORT: frontegg-proxy-port
        FRONTEGG_HTTP_CLIENT_PROXY_PROTO: frontegg-proxy-protocol

  externalSecret:
    enabled: true
    mountPath: /etc/config/config.yaml
    text: |
      {{- $secret := .contents | fromYaml}}
      apiKey: {{ $secret.frontegg.apiKeys.webhooksServiceApiKey | toYaml }}
      fronteggClientId: {{ $secret.frontegg.xxx.fronteggClientId | toYaml }}
      splitIO:
        splitIOKey: {{ $secret.externalServices.split.sdkKey | toYaml }}
      kafka:
        brokerList: {{ $secret.databases.kafka.brokerList | toYaml }}
        saslUsername: {{ $secret.databases.kafka.saslUserName | toYaml }}
        saslPassword: {{ $secret.databases.kafka.saslPassword | toYaml }}
      databases:
        mongoDB:
          uri: {{ $secret.databases.mongo.connectionString | toYaml }}
        mysql:
          host: {{ $secret.databases.generalMysql.host | toYaml }}
          username: {{ $secret.databases.generalMysql.username | toYaml }}
          password: {{ $secret.databases.generalMysql.password | toYaml }}
      internalServices:
        eventService:
          apiKey: {{ $secret.frontegg.apiKeys.eventsServiceApiKey | toYaml }}
      analytics:
        firehose:
          aws:
            keyId: {{ $secret.frontegg.analytics.firehoseAccessKeyId | toYaml }}
            secretAccessKey: {{ $secret.frontegg.analytics.firehoseSecretAccessKey | toYaml }}
            region: {{ $secret.frontegg.analytics.firehoseRegion | toYaml }}

    additionalSecrets: ""