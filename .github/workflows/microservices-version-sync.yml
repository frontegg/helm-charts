name: OnPrem Update Charts - Microservices Version Sync

on:
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Run in dry-run mode (no actual updates)'
        required: false
        default: false
        type: boolean

env:
  APPSTATE_REPO: 'frontegg/AppState'
  TERRAFORM_REPO: 'frontegg/terraform-private-env'
  WORKFLOW_NAME: 'Create Customer Environment'

jobs:
  extract-query-compare:
    name: Extract, Query & Compare Microservices Versions
    runs-on: ubuntu-latest
    outputs:
      has-updates: ${{ steps.compare.outputs.has-updates }}
      summary: ${{ steps.compare.outputs.summary }}
      updates-needed: ${{ steps.compare.outputs.updates-needed }}
    steps:
      - name: Checkout helm-charts repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'scripts/requirements.txt'

      - name: Install dependencies
        run: |
          pip install -r scripts/requirements.txt

      - name: Extract Services
        id: extract
        run: |
          python3 scripts/extract_services.py --verbose

      - name: Generate GitHub App Token
        id: app-token
        uses: actions/create-github-app-token@v1
        with:
          app-id: ${{ secrets.GH_APP_ID }}
          private-key: ${{ secrets.GH_APP_PRIVATE_KEY }}
          owner: frontegg
          repositories: AppState,terraform-private-env

      - name: Query AppState Repository
        id: query
        env:
          GITHUB_TOKEN: ${{ steps.app-token.outputs.token }}
        run: |
          python3 scripts/query_appstate.py --verbose

      - name: Compare Versions
        id: compare
        run: |
          python3 scripts/compare_versions.py --verbose

      - name: Upload comparison results
        uses: actions/upload-artifact@v4
        with:
          name: comparison-results
          path: comparison_results.json

  update-helm-values:
    name: Update Helm Values Files
    runs-on: ubuntu-latest
    needs: extract-query-compare
    if: needs.extract-query-compare.outputs.has-updates == 'true' && github.event.inputs.dry_run != 'true'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      - name: Download comparison results
        uses: actions/download-artifact@v4
        with:
          name: comparison-results

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r ${{ github.workspace }}/scripts/requirements.txt

      - name: Update Values Files
        run: |
          python3 scripts/update_values.py --verbose

      - name: Commit changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          if git diff --quiet; then
            echo "No changes to commit"
            exit 0
          fi
          
          git add charts/*/values.yaml
          
          # Generate commit message with updated services
          echo "chore: update microservices versions from AppState" > commit_msg.txt
          echo "" >> commit_msg.txt
          echo "Auto-updated microservice versions to match production AppState versions." >> commit_msg.txt
          echo "" >> commit_msg.txt
          echo "Updated services:" >> commit_msg.txt
          python3 -c 'import json; results = json.load(open("comparison_results.json", "r")); [print(f"- {update[\"service\"]}: {update[\"helm_version\"]} â†’ {update[\"appstate_version\"]}") for update in results["updates_needed"]]' >> commit_msg.txt
          
          git commit -F commit_msg.txt
          git push

  trigger-deployment:
    name: Trigger Deployment Workflow
    runs-on: ubuntu-latest
    needs: [extract-query-compare, update-helm-values]
    if: needs.extract-query-compare.outputs.has-updates == 'true' && github.event.inputs.dry_run != 'true'
    outputs:
      workflowRunId: ${{ steps.trigger.outputs.workflowRunId }}
      deployToken: ${{ steps.deploy-app-token.outputs.token }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Generate GitHub App Token for Deployment
        id: deploy-app-token
        uses: actions/create-github-app-token@v1
        with:
          app-id: ${{ secrets.GH_APP_ID }}
          private-key: ${{ secrets.GH_APP_PRIVATE_KEY }}
          owner: frontegg
          repositories: terraform-private-env

      - name: Trigger terraform-private-env workflow
        id: trigger
        uses: actions/github-script@v7
        with:
          github-token: ${{ steps.deploy-app-token.outputs.token }}
          script: |
            try {
              // Trigger the workflow
              const response = await github.rest.actions.createWorkflowDispatch({
                owner: 'frontegg',
                repo: 'terraform-private-env',
                workflow_id: 'Create Customer Environment',
                ref: 'main',
                inputs: {
                  reason: 'Automated microservices version sync',
                  triggered_by: 'helm-charts-version-sync'
                }
              });
              
              console.log('Successfully triggered deployment workflow');
              console.log('Response:', response.status);
              
              // Wait a moment for the workflow to start
              await new Promise(resolve => setTimeout(resolve, 5000));
              
              // Get the latest workflow run
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: 'frontegg',
                repo: 'terraform-private-env',
                workflow_id: 'Create Customer Environment',
                per_page: 1
              });
              
              if (runs.data.workflow_runs.length > 0) {
                const runId = runs.data.workflow_runs[0].id;
                console.log('Workflow run ID:', runId);
                core.setOutput('workflowRunId', runId);
              } else {
                console.log('No workflow runs found');
              }
              
            } catch (error) {
              console.error('Failed to trigger deployment workflow:', error);
              
              // Try with workflow file name if workflow name doesn't work
              try {
                const response = await github.rest.actions.createWorkflowDispatch({
                  owner: 'frontegg',
                  repo: 'terraform-private-env', 
                  workflow_id: 'create-customer-environment.yml',
                  ref: 'main',
                  inputs: {
                    reason: 'Automated microservices version sync',
                    triggered_by: 'helm-charts-version-sync'
                  }
                });
                
                console.log('Successfully triggered deployment workflow with file name');
                console.log('Response:', response.status);
                
                // Wait and get workflow run ID
                await new Promise(resolve => setTimeout(resolve, 5000));
                
                const runs = await github.rest.actions.listWorkflowRuns({
                  owner: 'frontegg',
                  repo: 'terraform-private-env',
                  workflow_id: 'create-customer-environment.yml',
                  per_page: 1
                });
                
                if (runs.data.workflow_runs.length > 0) {
                  const runId = runs.data.workflow_runs[0].id;
                  console.log('Workflow run ID:', runId);
                  core.setOutput('workflowRunId', runId);
                }
                
              } catch (secondError) {
                console.error('Failed to trigger deployment workflow with file name:', secondError);
                throw secondError;
              }
            }

  wait-for-deployment:
    name: Wait for Deployment Completion
    runs-on: ubuntu-latest
    needs: trigger-deployment
    if: needs['trigger-deployment'].outputs.workflowRunId != ''
    outputs:
      deploymentSuccess: ${{ steps.wait.outputs.success }}
    steps:
      - name: Wait for deployment workflow to complete
        id: wait
        uses: actions/github-script@v7
        with:
          github-token: ${{ needs['trigger-deployment'].outputs.deployToken }}
          script: |
            const runId = '${{ needs['trigger-deployment'].outputs.workflowRunId }}';
            const maxWaitTime = 30 * 60 * 1000; // 30 minutes
            const pollInterval = 30 * 1000; // 30 seconds
            const startTime = Date.now();
            
            console.log(`Waiting for workflow run ${runId} to complete...`);
            
            while (Date.now() - startTime < maxWaitTime) {
              try {
                const run = await github.rest.actions.getWorkflowRun({
                  owner: 'frontegg',
                  repo: 'terraform-private-env',
                  run_id: runId
                });
                
                const status = run.data.status;
                const conclusion = run.data.conclusion;
                
                console.log(`Workflow status: ${status}, conclusion: ${conclusion}`);
                
                if (status === 'completed') {
                  if (conclusion === 'success') {
                    console.log('âœ… Deployment workflow completed successfully');
                    core.setOutput('success', 'true');
                    return;
                  } else {
                    console.log(`âŒ Deployment workflow failed with conclusion: ${conclusion}`);
                    core.setOutput('success', 'false');
                    core.setFailed(`Deployment workflow failed: ${conclusion}`);
                    return;
                  }
                }
                
                // Wait before next poll
                await new Promise(resolve => setTimeout(resolve, pollInterval));
                
              } catch (error) {
                console.error('Error checking workflow status:', error);
                await new Promise(resolve => setTimeout(resolve, pollInterval));
              }
            }
            
            console.log('â° Timeout waiting for deployment workflow');
            core.setOutput('success', 'false');
            core.setFailed('Timeout waiting for deployment workflow to complete');

  create-release:
    name: Create GitHub Release
    runs-on: ubuntu-latest
    needs: [extract-query-compare, wait-for-deployment]
    if: needs['wait-for-deployment'].outputs.deploymentSuccess == 'true'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Download comparison results
        uses: actions/download-artifact@v4
        with:
          name: comparison-results

      - name: Generate release tag
        id: tag
        run: |
          # Generate tag based on current date and time
          TAG="sync-$(date +%Y%m%d-%H%M%S)"
          echo "tag=$TAG" >> $GITHUB_OUTPUT
          echo "Generated tag: $TAG"

      - name: Generate detailed release notes
        id: release-notes
        run: |
          # Create detailed release notes with version changes
          cat > release_notes.md << 'EOF'
          # ðŸš€ Microservices Version Sync
          
          **Release Date:** $(date '+%Y-%m-%d %H:%M:%S UTC')
          **Sync Tag:** ${{ steps.tag.outputs.tag }}
          
          This automated release synchronizes microservice versions between Helm charts and AppState production environment.
          
          ## ðŸ“Š Version Updates Summary
          
          EOF
          
          # Add detailed version table if comparison results exist
          if [ -f "comparison_results.json" ]; then
            echo "" >> release_notes.md
            python3 scripts/generate_summary_table.py >> release_notes.md
            echo "" >> release_notes.md
          fi
          
          # Add deployment information
          cat >> release_notes.md << 'EOF'
          
          ## ðŸ”„ Changes Made
          
          - âœ… **Extracted** microservices from Helm charts
          - âœ… **Queried** AppState repository for production versions  
          - âœ… **Compared** versions and identified updates needed
          - âœ… **Updated** Helm values files with new appVersion values
          - âœ… **Synchronized** versions with AppState production environment
          - âœ… **Triggered** deployment pipeline
          
          ## ðŸš€ Deployment Status
          
          âœ… **Deployment completed successfully**
          
          - **Workflow Run ID:** ${{ needs['trigger-deployment'].outputs.workflowRunId }}
          - **Deployment Pipeline:** Completed
          - **Status:** Success
          
          ## ðŸ“‹ Technical Details
          
          - **Workflow:** `${{ github.workflow }}`
          - **Run ID:** `${{ github.run_id }}`
          - **Commit:** `${{ github.sha }}`
          - **Branch:** `${{ github.ref_name }}`
          - **Triggered by:** `${{ github.actor }}`
          
          ## ðŸ”— Links
          
          - [Workflow Run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          - [Deployment Workflow](https://github.com/frontegg/terraform-private-env/actions/runs/${{ needs['trigger-deployment'].outputs.workflowRunId }})
          - [Repository](https://github.com/${{ github.repository }})
          
          ---
          
          *This release was automatically generated by the Microservices Version Sync workflow.*
          EOF
          
          # Set the release notes as output
          echo "notes<<EOF" >> $GITHUB_OUTPUT
          cat release_notes.md >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Create Release
        uses: softprops/action-gh-release@v2
        with:
          tag_name: ${{ steps.tag.outputs.tag }}
          name: "ðŸš€ Microservices Version Sync - ${{ steps.tag.outputs.tag }}"
          body: ${{ steps.release-notes.outputs.notes }}
          draft: false
          prerelease: false
          token: ${{ secrets.GITHUB_TOKEN }}

  notify-results:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [extract-query-compare, update-helm-values, trigger-deployment, wait-for-deployment, create-release]
    if: always()
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download comparison results
        uses: actions/download-artifact@v4
        with:
          name: comparison-results
        continue-on-error: true

      - name: Create Job Summary
        run: |
          echo "# Microservices Version Sync Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Debug: Check if comparison results file exists
          if [ -f "comparison_results.json" ]; then
            echo "âœ… Comparison results file found" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Comparison results file not found - some details may be missing" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.extract-query-compare.outputs.has-updates }}" == "true" ]; then
            echo "## âœ… Sync Completed Successfully" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The following actions were completed:" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… Extracted microservices from Helm charts" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… Queried AppState repository for production versions" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… Compared versions and identified updates needed" >> $GITHUB_STEP_SUMMARY
            
            if [ "${{ github.event.inputs.dry_run }}" != "true" ]; then
              echo "- âœ… Updated Helm values files" >> $GITHUB_STEP_SUMMARY
              echo "- âœ… Triggered deployment workflow" >> $GITHUB_STEP_SUMMARY
              
              if [ "${{ needs['wait-for-deployment'].outputs.deploymentSuccess }}" == "true" ]; then
                echo "- âœ… Deployment completed successfully" >> $GITHUB_STEP_SUMMARY
                echo "- âœ… Created GitHub release" >> $GITHUB_STEP_SUMMARY
              else
                echo "- âŒ Deployment failed or timed out" >> $GITHUB_STEP_SUMMARY
                echo "- â­ï¸ Skipped release creation" >> $GITHUB_STEP_SUMMARY
              fi
            else
              echo "- â­ï¸ Skipped updates (dry-run mode)" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "## â„¹ï¸ No Updates Needed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "All microservices are already up to date with AppState production versions." >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Add version comparison table if updates are available (for both dry-run and actual runs)
          if [ -f "comparison_results.json" ] && [ "${{ needs.extract-query-compare.outputs.has-updates }}" == "true" ]; then
            if [ "${{ github.event.inputs.dry_run }}" == "true" ]; then
              echo "## ðŸ“Š Planned Version Updates (Dry Run)" >> $GITHUB_STEP_SUMMARY
            else
              echo "## ðŸ“Š Version Updates Summary" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Generate the summary table using the dedicated script
            if [ "${{ github.event.inputs.dry_run }}" == "true" ]; then
              python3 scripts/generate_summary_table.py --dry-run >> $GITHUB_STEP_SUMMARY
            else
              python3 scripts/generate_summary_table.py >> $GITHUB_STEP_SUMMARY
            fi
            
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Add detailed summary if available
          if [ -f "comparison_results.json" ]; then
            echo "## ðŸ“‹ Detailed Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            python3 -c 'import json; results = json.load(open("comparison_results.json", "r")); print(results.get("summary", "Summary not available"))' >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "Summary not available - comparison results could not be loaded" >> $GITHUB_STEP_SUMMARY
          else
            echo "## ðŸ“‹ Detailed Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Comparison results file not found - this may indicate an issue with the comparison step" >> $GITHUB_STEP_SUMMARY
          fi
